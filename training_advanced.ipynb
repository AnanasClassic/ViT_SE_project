{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd726ab",
   "metadata": {},
   "source": [
    "# ViT Fine-tuning on CIFAR-10\n",
    "\n",
    "Training mini_vit architecture on CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "231c0e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66390f04",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a55f146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 150\n",
    "WARMUP_EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.05\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "PATCH_SIZE = 4\n",
    "IMG_SIZE = 32\n",
    "NUM_CLASSES = 10\n",
    "VIT_MLP_RATIO = 2\n",
    "CHANNEL = 192\n",
    "DEPTH = 12\n",
    "HEADS = 12\n",
    "\n",
    "MIXUP_ALPHA = 1.0\n",
    "CUTMIX_BETA = 1.0\n",
    "MIX_PROB = 0.5\n",
    "RANDOM_ERASING_PROB = 0.25\n",
    "REPEATED_AUG = 3\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda')\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b12d66",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b23345",
   "metadata": {},
   "source": [
    "### Data Augmentation Techniques\n",
    "\n",
    "- **RandomHorizontalFlip** and **RandomCrop**: Basic spatial augmentations\n",
    "\n",
    "- **AutoAugment (CIFAR10)**: Learned augmentation policy optimized for CIFAR-10\n",
    "\n",
    "- **RandomErasing**: Randomly masks rectangular regions to improve robustness\n",
    "\n",
    "- **Normalization**: Using CIFAR-10 dataset statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f72a347",
   "metadata": {},
   "source": [
    "### Repeated Augmentation\n",
    "\n",
    "Repeating each sample 3x with different augmentations per epoch to increase training diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38c69f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vlad/projects/vit_se_project/.venv/lib/python3.11/site-packages/torchvision/datasets/cifar.py:83: VisibleDeprecationWarning: dtype(): align should be passed as Python or NumPy boolean but got `align=0`. Did you mean to pass a tuple to create a subarray type? (Deprecated NumPy 2.4)\n",
      "  entry = pickle.load(f, encoding=\"latin1\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset size: 10000\n",
      "Train batches: 585\n",
      "Test batches: 40\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import AutoAugment, AutoAugmentPolicy, RandomErasing\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "cifar_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar_std = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(IMG_SIZE, padding=4),\n",
    "    AutoAugment(policy=AutoAugmentPolicy.CIFAR10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar_mean, std=cifar_std),\n",
    "    RandomErasing(p=RANDOM_ERASING_PROB, scale=(0.02, 0.4), ratio=(0.3, 3.3), value='random'),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar_mean, std=cifar_std),\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "class RepeatedAugmentationSampler(Sampler):\n",
    "    def __init__(self, dataset, num_repeats=1, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.num_repeats = num_repeats\n",
    "        self.shuffle = shuffle\n",
    "        self.num_samples = len(dataset) * num_repeats\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = list(range(len(self.dataset)))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(indices)\n",
    "        \n",
    "        indices = indices * self.num_repeats\n",
    "        if self.shuffle:\n",
    "            random.shuffle(indices)\n",
    "        \n",
    "        return iter(indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "train_sampler = RepeatedAugmentationSampler(train_dataset, num_repeats=REPEATED_AUG, shuffle=True)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sampler=train_sampler,\n",
    "    num_workers=2,\n",
    "    drop_last=True\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82074cd",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7df4bf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: MiniViT\n",
      "Trainable parameters: 3,562,378\n",
      "Image size: 32x32\n",
      "Patch size: 4\n",
      "Dim: 192\n",
      "Depth: 12\n",
      "Heads: 12\n",
      "MLP ratio: 2\n"
     ]
    }
   ],
   "source": [
    "from src.models.mini_vit import MiniViT\n",
    "\n",
    "model = MiniViT(\n",
    "    img_size=IMG_SIZE,\n",
    "    patch_size=PATCH_SIZE,\n",
    "    n_classes=NUM_CLASSES,\n",
    "    emb_dim=CHANNEL,\n",
    "    n_layers=DEPTH,\n",
    "    n_heads=HEADS,\n",
    "    mlp_hidden_dim=CHANNEL * VIT_MLP_RATIO,\n",
    ")\n",
    "\n",
    "model.to(DEVICE)\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model: MiniViT\")\n",
    "print(f\"Trainable parameters: {num_params:,}\")\n",
    "print(f\"Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Patch size: {PATCH_SIZE}\")\n",
    "print(f\"Dim: {CHANNEL}\")\n",
    "print(f\"Depth: {DEPTH}\")\n",
    "print(f\"Heads: {HEADS}\")\n",
    "print(f\"MLP ratio: {VIT_MLP_RATIO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0974ebb",
   "metadata": {},
   "source": [
    "## Loss, Optimizer, Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52544a7",
   "metadata": {},
   "source": [
    "### Training Regularization & Optimization\n",
    "\n",
    "- **Label Smoothing** (0.1): Prevents overconfident predictions\n",
    "\n",
    "- **Weight Decay** (0.05): L2 regularization in AdamW optimizer\n",
    "\n",
    "- **Cosine Annealing with Warmup**: \n",
    "  - Linear warmup for 15 epochs to stabilize training\n",
    "  - Cosine decay schedule for remaining epochs to fine-tune convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d5501ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Criterion: CrossEntropyLoss with label smoothing=0.1\n",
      "Optimizer: AdamW(lr=0.001, wd=0.05)\n",
      "Scheduler: CosineAnnealingWithWarmup\n",
      "Total steps: 87750\n",
      "Warmup steps: 8775\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "warmup_steps = len(train_loader) * WARMUP_EPOCHS\n",
    "\n",
    "class CosineAnnealingWithWarmup:\n",
    "    def __init__(self, optimizer, warmup_steps, total_steps, lr):\n",
    "        self.optimizer = optimizer\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "        self.lr = lr\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def step(self):\n",
    "        self.step_count += 1\n",
    "        \n",
    "        if self.step_count < self.warmup_steps:\n",
    "            lr = self.lr * (self.step_count / self.warmup_steps)\n",
    "        else:\n",
    "            progress = (self.step_count - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
    "            lr = self.lr * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "        \n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        \n",
    "        return lr\n",
    "\n",
    "scheduler = CosineAnnealingWithWarmup(optimizer, warmup_steps, total_steps, LEARNING_RATE)\n",
    "\n",
    "print(f\"Criterion: CrossEntropyLoss with label smoothing={LABEL_SMOOTHING}\")\n",
    "print(f\"Optimizer: AdamW(lr={LEARNING_RATE}, wd={WEIGHT_DECAY})\")\n",
    "print(f\"Scheduler: CosineAnnealingWithWarmup\")\n",
    "print(f\"Total steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4e1e5d",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8e7a5",
   "metadata": {},
   "source": [
    "### MixUp & CutMix\n",
    "\n",
    "**MixUp**: Blends two images and their labels with random ratio  \n",
    "\n",
    "**CutMix**: Cuts and pastes patches between images, mixing labels proportionally\n",
    "\n",
    "**Mix Probability**: 50% chance to apply either MixUp or CutMix during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5b61f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions ready\n"
     ]
    }
   ],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def cutmix_data(x, y, beta=1.0):\n",
    "    lam = np.random.beta(beta, beta) if beta > 0 else 1\n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "    \n",
    "    W = x.size(2)\n",
    "    H = x.size(3)\n",
    "    \n",
    "    cut_ratio = np.sqrt(1.0 - lam)\n",
    "    cut_h = int(H * cut_ratio)\n",
    "    cut_w = int(W * cut_ratio)\n",
    "    \n",
    "    cx = np.random.randint(0, W)\n",
    "    cy = np.random.randint(0, H)\n",
    "    \n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "    \n",
    "    x[..., bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]\n",
    "    \n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1)) / (H * W)\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def train_one_epoch(epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\", leave=False)\n",
    "    \n",
    "    for images, targets in pbar:\n",
    "        images = images.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        \n",
    "        r = np.random.rand()\n",
    "        if r < MIX_PROB:\n",
    "            choice = np.random.rand()\n",
    "            if choice < 0.5:\n",
    "                images, targets_a, targets_b, lam = cutmix_data(images, targets, CUTMIX_BETA)\n",
    "            else:\n",
    "                images, targets_a, targets_b, lam = mixup_data(images, targets, MIXUP_ALPHA)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "        else:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr = scheduler.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, pred = outputs.max(1)\n",
    "            correct = pred.eq(targets).sum().item()\n",
    "            total_correct += correct\n",
    "            total_samples += targets.size(0)\n",
    "            total_loss += loss.item() * targets.size(0)\n",
    "        \n",
    "        acc = 100.0 * correct / targets.size(0)\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{acc:.2f}%', 'lr': f'{lr:.6f}'})\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = 100.0 * total_correct / total_samples\n",
    "    \n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "def validate(loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in loader:\n",
    "            images = images.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            _, pred = outputs.max(1)\n",
    "            correct = pred.eq(targets).sum().item()\n",
    "            \n",
    "            total_loss += loss.item() * targets.size(0)\n",
    "            total_correct += correct\n",
    "            total_samples += targets.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc = 100.0 * total_correct / total_samples\n",
    "    \n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "print(\"Training functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb09169",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f83cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/150 [Train]:   0%|          | 0/585 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention shapes - Q: torch.Size([256, 12, 65, 16]), K: torch.Size([256, 12, 65, 16]), V: torch.Size([256, 12, 65, 16]), A: torch.Size([256, 12, 65, 65])\n",
      "Attention output shape: torch.Size([256, 65, 192])\n",
      "Attention output shape: torch.Size([256, 65, 192])\n",
      "Attention output shape: torch.Size([256, 65, 192])\n",
      "Attention output shape: torch.Size([256, 65, 192])\n",
      "Attention output shape: torch.Size([256, 65, 192])\n",
      "Attention output shape: torch.Size([256, 65, 192])\n",
      "Attention output shape: torch.Size([256, 65, 192])\n",
      "Attention output shape: torch.Size([256, 65, 192])\n",
      "Attention output shape: torch.Size([256, 65, 192])\n",
      "Attention output shape: torch.Size([256, 65, 192])\n",
      "Attention output shape: torch.Size([256, 65, 192])\n",
      "Attention output shape: torch.Size([256, 65, 192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   5/150 | Train Loss: 1.7534 | Train Acc: 42.42% | Val Loss: 1.2864 | Val Acc: 64.35%\n",
      "Best model saved! Accuracy: 64.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  10/150 | Train Loss: 1.5832 | Train Acc: 50.68% | Val Loss: 1.1000 | Val Acc: 73.11%\n",
      "Best model saved! Accuracy: 73.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  15/150 | Train Loss: 1.5049 | Train Acc: 54.41% | Val Loss: 0.9624 | Val Acc: 79.79%\n",
      "Best model saved! Accuracy: 79.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  20/150 | Train Loss: 1.4157 | Train Acc: 58.37% | Val Loss: 0.9193 | Val Acc: 81.42%\n",
      "Best model saved! Accuracy: 81.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  25/150 | Train Loss: 1.3258 | Train Acc: 63.25% | Val Loss: 0.8505 | Val Acc: 84.82%\n",
      "Best model saved! Accuracy: 84.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  30/150 | Train Loss: 1.2980 | Train Acc: 64.44% | Val Loss: 0.7975 | Val Acc: 87.06%\n",
      "Best model saved! Accuracy: 87.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  35/150 | Train Loss: 1.3006 | Train Acc: 64.56% | Val Loss: 0.7673 | Val Acc: 88.53%\n",
      "Best model saved! Accuracy: 88.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  40/150 | Train Loss: 1.2581 | Train Acc: 64.61% | Val Loss: 0.7450 | Val Acc: 89.44%\n",
      "Best model saved! Accuracy: 89.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  45/150 | Train Loss: 1.2286 | Train Acc: 66.89% | Val Loss: 0.7368 | Val Acc: 90.09%\n",
      "Best model saved! Accuracy: 90.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50/150 | Train Loss: 1.2130 | Train Acc: 67.08% | Val Loss: 0.7248 | Val Acc: 90.39%\n",
      "Best model saved! Accuracy: 90.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  55/150 | Train Loss: 1.1970 | Train Acc: 68.15% | Val Loss: 0.7253 | Val Acc: 90.57%\n",
      "Best model saved! Accuracy: 90.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  60/150 | Train Loss: 1.1462 | Train Acc: 71.08% | Val Loss: 0.7107 | Val Acc: 91.69%\n",
      "Best model saved! Accuracy: 91.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  65/150 | Train Loss: 1.1536 | Train Acc: 69.93% | Val Loss: 0.7167 | Val Acc: 91.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  70/150 | Train Loss: 1.0991 | Train Acc: 72.79% | Val Loss: 0.7076 | Val Acc: 91.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  75/150 | Train Loss: 1.1147 | Train Acc: 72.26% | Val Loss: 0.6852 | Val Acc: 92.00%\n",
      "Best model saved! Accuracy: 92.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  80/150 | Train Loss: 1.1044 | Train Acc: 73.07% | Val Loss: 0.6831 | Val Acc: 92.62%\n",
      "Best model saved! Accuracy: 92.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  85/150 | Train Loss: 1.0830 | Train Acc: 73.47% | Val Loss: 0.6797 | Val Acc: 92.97%\n",
      "Best model saved! Accuracy: 92.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  90/150 | Train Loss: 1.0787 | Train Acc: 73.90% | Val Loss: 0.6873 | Val Acc: 92.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  95/150 | Train Loss: 1.0522 | Train Acc: 75.34% | Val Loss: 0.6780 | Val Acc: 93.25%\n",
      "Best model saved! Accuracy: 93.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/150 | Train Loss: 1.0531 | Train Acc: 75.06% | Val Loss: 0.6722 | Val Acc: 93.66%\n",
      "Best model saved! Accuracy: 93.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/150 | Train Loss: 1.0630 | Train Acc: 74.84% | Val Loss: 0.6621 | Val Acc: 93.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/150 | Train Loss: 1.0127 | Train Acc: 76.65% | Val Loss: 0.6664 | Val Acc: 93.74%\n",
      "Best model saved! Accuracy: 93.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/150 | Train Loss: 1.0370 | Train Acc: 75.91% | Val Loss: 0.6503 | Val Acc: 94.37%\n",
      "Best model saved! Accuracy: 94.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/150 | Train Loss: 1.0298 | Train Acc: 74.78% | Val Loss: 0.6437 | Val Acc: 94.83%\n",
      "Best model saved! Accuracy: 94.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/150 | Train Loss: 1.0106 | Train Acc: 75.51% | Val Loss: 0.6463 | Val Acc: 94.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 130/150 | Train Loss: 0.9818 | Train Acc: 77.56% | Val Loss: 0.6439 | Val Acc: 94.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/150 | Train Loss: 0.9855 | Train Acc: 76.86% | Val Loss: 0.6440 | Val Acc: 94.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/150 | Train Loss: 1.0057 | Train Acc: 75.62% | Val Loss: 0.6432 | Val Acc: 94.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/150 | Train Loss: 0.9973 | Train Acc: 77.33% | Val Loss: 0.6431 | Val Acc: 94.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/150 | Train Loss: 0.9805 | Train Acc: 77.74% | Val Loss: 0.6429 | Val Acc: 94.71%\n",
      "\n",
      "Training completed! Best accuracy: 94.83%\n",
      "Final Test Accuracy: 94.71%\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('checkpoints', exist_ok=True)\n",
    "\n",
    "best_acc = 0\n",
    "eval_interval = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_one_epoch(epoch)\n",
    "\n",
    "    val_loss, val_acc = validate(test_loader)\n",
    "    \n",
    "    if (epoch + 1) % eval_interval == 0 or epoch == EPOCHS - 1:\n",
    "        print(f\"Epoch {epoch+1:3d}/{EPOCHS} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "            f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_adcc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_acc': best_acc,\n",
    "            }, 'checkpoints/best.pt')\n",
    "            print(f\"Best model saved! Accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "print(f\"\\nTraining completed! Best accuracy: {best_acc:.2f}%\")\n",
    "\n",
    "# Final test on test set\n",
    "test_loss, test_acc = validate(test_loader)\n",
    "print(f\"Final Test Accuracy: {test_acc:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vit-se-project)",
   "language": "python",
   "name": "vit-se-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
